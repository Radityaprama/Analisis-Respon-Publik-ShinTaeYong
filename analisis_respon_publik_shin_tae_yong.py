# -*- coding: utf-8 -*-
"""Analisis Respon Publik Shin Tae Yong.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WAxZKBj9O5BHLa_aN61qgS4IHRoLrUtq
"""

# Step 1: Install Library yang Dibutuhkan
#!pip install networkx matplotlib pandas

# Step 2: Import Library
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from networkx.algorithms.community import greedy_modularity_communities

# Step 3: Upload File CSV
#from google.colab import files

# Step 4: Membaca Dataset
# Pastikan nama file sesuai dengan yang diunggah
file_name = "source_target.csv"  # Ganti nama file jika berbeda
data = pd.read_csv(file_name)

# Melihat beberapa baris data
print("Contoh data:")
print(data)

df = pd.DataFrame(data)
G = nx.from_pandas_edgelist(df, 'Source', 'Target')

# Count the number of nodes and edges
num_nodes = G.number_of_nodes()
num_edges = G.number_of_edges()

# Output the results
print(f"Number of Nodes: {num_nodes}")
print(f"Number of Edges: {num_edges}")

# Step 5: Membuat Graph dari Hubungan Fans
# Asumsi dataset memiliki kolom 'Fan1' dan 'Fan2' yang menunjukkan hubungan antar fans
G = nx.Graph()

# Tambahkan edges ke graph dari dataset
for _, row in data.iterrows():
    G.add_edge(row["Source"], row["Target"])

# Step 6: Visualisasi Graph
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G)  # Tata letak menggunakan spring layout
nx.draw(
    G, pos,
    with_labels=True,
    node_color="lightgreen",
    node_size=3000,
    font_size=10,
    font_weight="bold",
    edge_color="blue"
)
plt.title("Graph Hubungan Antar Fans")
plt.show()

# Step 7: Menyimpan Graph ke File (Opsional)
# Ekspor graph ke format GEXF untuk digunakan di Gephi
nx.write_gexf(G, "graph_fans_shin_tae_yong.gexf")
print("Graph berhasil diekspor ke file 'graph_fans_shin_tae_yong.gexf'")

G = nx.DiGraph()
G.add_edges_from(zip(data['Source'], data['Target']))

# Draw the graph
plt.figure(figsize=(10, 7))
nx.draw(G, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')
plt.title("Graph Visualization")
plt.show()

# Compute centrality metrics
print("\nCentrality Metrics:")
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)

# Convert centrality metrics to DataFrame for easier visualization
centrality_df = pd.DataFrame({
    'Node': list(G.nodes),
    'Degree Centrality': [degree_centrality[node] for node in G.nodes],
    'Betweenness Centrality': [betweenness_centrality[node] for node in G.nodes],
    'Closeness Centrality': [closeness_centrality[node] for node in G.nodes]
})

# Display the centrality metrics
print(centrality_df)

# Save the centrality metrics to a CSV file
output_path = "/content/centrality_metrics.csv"
centrality_df.to_csv(output_path, index=False)
print(f"\nCentrality metrics saved to {output_path}")

data = data.dropna(subset=['Source', 'Target', 'Timestamp'])

# Build a directed graph (use nx.Graph() for an undirected graph)
G = nx.DiGraph()
G.add_edges_from(zip(data['Source'], data['Target']))

# Compute centrality metrics
degree_centrality = nx.degree_centrality(G)
betweenness_centrality = nx.betweenness_centrality(G)
closeness_centrality = nx.closeness_centrality(G)

# Find the most and least central nodes
most_centrality_node = max(degree_centrality, key=degree_centrality.get)
least_centrality_node = min(degree_centrality, key=degree_centrality.get)

most_betweenness_node = max(betweenness_centrality, key=betweenness_centrality.get)
least_betweenness_node = min(betweenness_centrality, key=betweenness_centrality.get)

most_closeness_node = max(closeness_centrality, key=closeness_centrality.get)
least_closeness_node = min(closeness_centrality, key=closeness_centrality.get)

# Display the results
print("\nResults:")
print(f"Most Centrality: {most_centrality_node}")
print(f"Least Centrality: {least_centrality_node}")

print(f"Most Betweenness: {most_betweenness_node}")
print(f"Least Betweenness: {least_betweenness_node}")

print(f"Most Closeness: {most_closeness_node}")
print(f"Least Closeness: {least_closeness_node}")

# Build an undirected graph (communities are typically detected on undirected graphs)
G = nx.Graph()
# Ensure the nodes are represented as strings for comparability
G.add_edges_from([(str(row['Source']), str(row['Target'])) for _, row in data.iterrows()])

# Detect communities using the greedy modularity algorithm
communities = list(greedy_modularity_communities(G))

# Display the detected communities
print("\nCommunities:")
for i, community in enumerate(communities):
    print(f"Community {i + 1}: {sorted(community)}")

# Save communities to a file
community_df = pd.DataFrame({
    'Community': [f"Community {i + 1}" for i in range(len(communities))],
    'Nodes': [sorted(list(community)) for community in communities]
})

# Calculate the most and least interacted users
interaction_counts = data['Source'].value_counts() + data['Target'].value_counts()
most_interacted_user = interaction_counts.idxmax()
least_interacted_user = interaction_counts.idxmin()

# Display the results
print("\nInteraction Results:")
print(f"Most Interacted User: {most_interacted_user} with {interaction_counts[most_interacted_user]} interactions")
print(f"Least Interacted User: {least_interacted_user} with {interaction_counts[least_interacted_user]} interactions")

# Convert 'Timestamp' to datetime with a specified format
data['Timestamp'] = pd.to_datetime(data['Timestamp'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')

# Drop rows where 'Timestamp' could not be converted
data = data.dropna(subset=['Timestamp'])

# Find the newest and oldest tweets
newest_tweet = data.loc[data['Timestamp'].idxmax()]
oldest_tweet = data.loc[data['Timestamp'].idxmin()]

# Display the newest and oldest tweets
print("\nTimestamp Results:")
print(f"Newest Tweet: {newest_tweet['Timestamp']} by {newest_tweet['Source']} -> {newest_tweet['Target']}")
print(f"Oldest Tweet: {oldest_tweet['Timestamp']} by {oldest_tweet['Source']} -> {oldest_tweet['Target']}")

#!pip install transformers textblob

import pandas as pd
from transformers import pipeline
from textblob import TextBlob

# Load your Twitter data into a Pandas DataFrame
data = pd.read_csv('Shin_Tae-yong.csv')

# Check the actual column names in